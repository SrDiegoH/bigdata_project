{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker_pyspark\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField,StringType, FloatType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import create_map, struct\n",
    "from sagemaker_pyspark import classpath_jars\n",
    "#from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANDED_PATH = 'F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\landed\\\\'\n",
    "MODELED_PATH = \"F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\modeled\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Acidentes de Transito\")\n",
    "builder.config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "builder.config(\"spark.speculation\", \"false\")\n",
    "builder.config(\"spark.sql.parquet.compression.codec\", \"gzip\")\n",
    "builder.config(\"spark.debug.maxToStringFields\", \"100\")\n",
    "builder.config(\"spark.driver.extraClassPath\", classpath)\n",
    "builder.config(\"spark.driver.memory\", \"1g\")\n",
    "builder.config(\"spark.driver.cores\", \"1\")\n",
    "builder.config(\"spark.executor-memory\", \"20g\")\n",
    "builder.config(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "builder.master(\"local[*]\")\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vitimas_info(df):\n",
    "    if \"vitimasfatais\" in df and df['tipo']=='VÍTIMA FATAL':\n",
    "        df['vitimas'] = df['vitimasfatais']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_vitimas_fatais(df):\n",
    "    if \"vitimasfatais\" in df:\n",
    "        df = df.drop(\"vitimasfatais\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dafault_value(df):\n",
    "    for coluna in ['auto', 'moto', 'ciclom', 'ciclista', 'pedestre', 'onibus', 'caminhao', 'viatura', 'outros', 'vitimas']:\n",
    "        if coluna in df:\n",
    "            df[coluna] = pd.to_numeric(df[coluna], errors='coerce').fillna(0.0, inplace=False)\n",
    "            \n",
    "    if 'data' in df:\n",
    "        df['data'] = pd.to_datetime(df['data'], errors='coerce').dt.strftime('%m/%d/%Y')\n",
    "        \n",
    "    if 'hora' in df:\n",
    "        df['hora'] = pd.to_datetime(df['hora'], errors='coerce').dt.strftime(\"%H:%M\")\n",
    "        \n",
    "    df['situacao'] = df['situacao'].replace('Err:512', 'SEM_DADO', regex=True)\n",
    "    df['situacao'] = df['situacao'].replace('', 'SEM_DADO', regex=True)\n",
    "    \n",
    "    df = df.fillna('SEM_DADO', inplace=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\landed\\\\acidentes-transito-2015.csv',\n",
       " 'F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\landed\\\\acidentes-transito-2016.csv',\n",
       " 'F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\landed\\\\acidentes2017.csv',\n",
       " 'F:\\\\Sistema\\\\Downloads\\\\bigDataProjefct\\\\landed\\\\acidentes2018.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataset = [f'{LANDED_PATH}{file}' for file in os.listdir(LANDED_PATH) if '.csv' in file]\n",
    "all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset_list = spark.sparkContext.parallelize(all_dataset)\\n.map(lambda file: pd.read_csv(file, sep=\\';\\'))\\n.map(lambda df: df[df[\\'situacao\\'] != \\'CANCELADA\\'])\\n.map(lambda df: df.apply(lambda row: row.strip() if isinstance(row, str) else row, axis=1))\\n.map(lambda df: df.drop([\"endereco\", \"numero\", \"complemento\", \"descricao\"], axis=1))\\n.map(lambda df: df.apply(lambda row: merge_vitimas_info(row), axis=1))\\n.map(lambda df: df.apply(lambda row: drop_vitimas_fatais(row), axis=1))\\n.map(lambda df: set_dafault_value(df))\\n.reduce(lambda df_1, df_2: df_1.append(df_2))\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dataset_list = spark.sparkContext.parallelize(all_dataset)\n",
    ".map(lambda file: pd.read_csv(file, sep=';'))\n",
    ".map(lambda df: df[df['situacao'] != 'CANCELADA'])\n",
    ".map(lambda df: df.apply(lambda row: row.strip() if isinstance(row, str) else row, axis=1))\n",
    ".map(lambda df: df.drop([\"endereco\", \"numero\", \"complemento\", \"descricao\"], axis=1))\n",
    ".map(lambda df: df.apply(lambda row: merge_vitimas_info(row), axis=1))\n",
    ".map(lambda df: df.apply(lambda row: drop_vitimas_fatais(row), axis=1))\n",
    ".map(lambda df: set_dafault_value(df))\n",
    ".reduce(lambda df_1, df_2: df_1.append(df_2, ignore_index=True))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = spark.sparkContext.parallelize(all_dataset).map(lambda file: pd.read_csv(file, sep=';')).map(lambda df: df[df['situacao'] != 'CANCELADA']).map(lambda df: df.apply(lambda row: row.strip() if isinstance(row, str) else row, axis=1)).map(lambda df: df.drop([\"endereco\", \"numero\", \"complemento\", \"descricao\"], axis=1)).map(lambda df: df.apply(lambda row: merge_vitimas_info(row), axis=1)).map(lambda df: df.apply(lambda row: drop_vitimas_fatais(row), axis=1)).map(lambda df: set_dafault_value(df)).reduce(lambda df_1, df_2: df_1.append(df_2, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_list.to_csv(f'{MODELED_PATH}final_dataset.csv', sep=';', encoding='utf-8')\n",
    "dataset_list.to_parquet(f'{MODELED_PATH}final_dataset.parquet', engine='fastparquet', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_csv(all_dataset[1], sep=';')\n",
    "df = df[df['situacao'] != 'CANCELADA']\n",
    "df = df.drop([\"endereco\", \"numero\", \"complemento\", \"descricao\"], axis=1)\n",
    "df = df.apply(lambda row: merge_vitimas_info(row), axis=1)\n",
    "df = df.apply(lambda row: drop_vitimas_fatais(row), axis=1)\n",
    "df = df.apply(lambda row: row.strip() if isinstance(row, str) else row, axis=1)\n",
    "#df = set_dafault_value(df)\n",
    "#df.to_csv(f'{LANDED}aaaaaa.csv', sep=';', encoding='utf-8')\n",
    "df = df[df['bairro'] == 'BOA VIAGEM']\n",
    "df = df[df['situacao'] == 'PENDENTE']\n",
    "df = df[df['tipo'] == 'FISCALIZAÇÃO']\n",
    "df = df[df['data'] == '2016-04-02']\n",
    "#df = df[df['data'] == '02/04/2016']\n",
    "#df.loc[5]\n",
    "df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
